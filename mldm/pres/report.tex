%
% $Id$ 
%
% $LastChangedDate$ 
% 
% $LastChangedBy$
%

\documentclass[10pt]{unbthesis}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{setspace}			
\onehalfspacing

\title{CS6735 Research Project: Review of Machine Learning Evaluation
  Methods beyond Accuracy}
\author{Justin Kamerman 3335272}
\date{\today}

\begin{document}
\maketitle
% No chapter numbers
\renewcommand*\thesection{\arabic{section}}

%----------------------------------------
% Abstract
%----------------------------------------
\section*{Abstract}
There exists a variety of measures for evaluating machine
learning classifier performance. The prominent measures are examined
and appropriate and inappropriate scenarios for their use
identified. Scalar as well as non-scalar evaluation methods are
compared and the use of statistical techniques to estimate error rates
and confidence levels is analyzed.

\section{Introduction}
Key to the field of machine learning is the ability to evaluate a
classifiers performance objectively and extrapolate performance
metrics to predict how a classifier will perform on a previously
unseen inputs. 

Few machine learning algorithms offer the luxury of being analytically
comprehensible e.g. Decision Trees. Such algorithms
allow researchers to analytically predict and compare classifier
performance. In most cases, however, researchers must rely on
empirical methods to evaluate machine learning algorithms. It is
important for the field in general to establish a well defined set of
methods for comparing machine learning methods. If researchers cannot
convince others that they have discovered a better algorithm, or do
convince others with unreliable evaluation techniques, the field risks
excursions down dead-end paths of research or abandonment of those to
potential breakthroughs. 

The ability to evaluate a classifier suitably is not
only important for comparing one classifier or one algorithm to
another but also in the generation classifiers themselves. In
principal, machine learning algorithms involve the search of a
\textit{hypothesis space} for an optimal target function that best
fits the training data. In many cases, this hypothesis space search
involves an explicit comparison of the performance of competing target
function candidates. The ability to pick the best candidate for the
job is at the heart of the success, or failure, of a machine learning
algorithm. If such methods cannot converge on an optimal (local or
global) target function, their will be of limited utility in
practise.

In this paper we examine the prevalent evaluation metrics in use by
machine language researchers as well as their origins and shortcomings as
applied in the field. We track the development of new methods which,
in most cases, extend and complement existing. We identify domain
specific factors which effect different metrics and should be
considered when selecting a particular evaluation method. In
particular, we look at how the structure and composition of the
training set should effect the choice of evaluation methods.

While our discussion will deal with the evaluation of binary
classifiers, the same arguments apply in principal to multiclass
classifiers. In effect, any multiclass classifier can be reduced to a
binary classification problem by decomposition of the hypothesis. 


\section{What's Wrong with Accuracy ?}
Accuracy is the simplest and most intuitive evaluation measure for
classifiers \cite{RefWorks:43}. Simply stated, accuracy, \(A\), is the
count of how many mistakes classification mistakes made over a finite
set of instances:

\begin{equation}
\label{equ:accuracy}
A = \frac{T_p + T_n}{T_p + F_p + F_n + T_n}
\end{equation}

where \(t_p\) is the number of \textit{true positive} classifications
i.e. positive training instances that were correctly classified as
positive; \(t_n\) is the number of \textit{true negative}
classifications i.e. negative training instances that were correctly
classified as such. \(P\) is the total number of positive training
example and \(N\) the total number of negative training examples.

Accuracy does not distinguish the types of errors it makes (false
positives vs true negatives). While this is acceptable if the evaluation
data set contains as many examples of both classes (i.e. it is
balanced), for most real-world problems one type of misclassification
error is much more expensive than another \cite{RefWorks:45}. Accuracy
maximization also assumes that the class distribution is known for the
target environment. Without knowledge of target class distribution we
cannot be sure that we are actually maximizing accuracy for the
problem domain from which the data set was drawn \cite{RefWorks:45}.


\section{The Confusion Matrix}
\label{sec:confusionmatrix}
Essentially any training set is a labelled sample drawn from a
probability distribution of possible instances in the problem
domain. Useful machine learning application are typically given a
sample of labelled instances from which to infer the probability
distribution of a large population. How representative
this sample is of the underlying population is key to how well our
classifiers can hope to perform classifying the rest of the instance
population. The structure of the training set effects different evaluation
metrics differently and, by implication, the target function upon
which they converge if the metric is used for training. Bea cause of
this, it is
important to analyze evaluation methods 
in terms of their variability with respect to the structure of the
training set. Certain metrics will be better suited to certain
situations depending on the composition of the training set. The basis
of this analysis of variability is a structure called the
\textit{confusion matrix}.

The raw data produced by a classification scheme during testing are
counts of the correct and incorrect classifications from each
class. This information is normally displayed in a \textit{confusion
  matrix} showing the difference between the true and predicted
classes for a set of labelled examples as shown in Table
\ref{tab:confusionmatrix}. In Table \ref{tab:confusionmatrix}, \(t_p\)
and \(t_n\) are the number of true positives and true negatives
respectively. \(f_p\) and \(f_n\) are the number of false positives
and false negatives respectively. The row totals, \(C_p\) and \(C_n\),
are the number of predicted positive and negative examples, and the
column totals, \(P\) and \(N\), are the number of actual positive
and negative examples. 

\begin{table}
\centering
  \begin{tabular}{c|c|c|}
    & \multicolumn{2}{|c|}{Actual} \\ \cline{1-3}
 Predicted & +                   & -                  \\ \hline
    +      & \(T_p\)             & \(F_p\)             \\ \hline
    -      & \(F_n\)             & \(T_n\)             \\ \hline
           & \(P = T_p + F_n\)   & \(N = F_p + T_n\)    \\ \cline{2-3}
  \end{tabular}
  \caption{A confusion matrix. \(T_p\) = true positive count, \(F_n\)
  = false negative count, \(F_p\) = false positive count, and \(T_n\)
  is true negative count, \(P\) = actual positive count, \(N\) = actual
  negative count.}
  \label{tab:confusionmatrix}
\end{table}

Most machine learning evaluation metrics are derived from the raw
values in the \textit{confusion matrix}. Depending on how they combine
these raw values, they will be effected by changes in
therein. Analyzing the invariance of machine learning measures with
respect to changes in the \textit{confusion matrix} gives
insight into the suitability of the metrics to different problem
domains \cite{RefWorks:36}. Invariance of a measure to a specific
\textit{confusion matrix} transform indicates that it does not detect
this change. Depending on the learning goals, invariance can be
beneficial or adverse. The following invariance properties effect the
a measures applicability and trustworthiness \cite{RefWorks:36}:

\begin{itemize}
  \item \textbf{p1:} invariance under the change of \(T_p\) with
    \(T_n\) and \(F_n\) with \(F_p\). This property reflects a
    measure's permanence with respect to classification results
    distribution. The measure may not recognize asymmetry of
    classification results and may not be trustworthy when classifiers
    are compared on data sets unbalanced class distributions. On the
    other hand, this invariance makes \textit{accuracy} a robust
    measure for an algorithm's overall performance and insensitive to
    performance on a specific class \cite{RefWorks:33}.

  \item \textbf{p2:} invariance under a change in \(T_n\) only. This
    property reflects that a measure does not recognize specifying
    abilities of classifiers and may be more applicable to domains
    with a weak negative class.
  
  \item \textbf{p3:} invariance under a change in \(F_p\) only. This
    property indicates a measure's constancy if disagreement increases
    between the data and classifier labels. A non-invariant measure
    may not be suitable for data with many counter examples.

  \item \textbf{p4:} invariance under the classification scaling \(T_p
    \rightarrow k_1 T_p; T_n \rightarrow k_2 T_n; F_p \rightarrow k_1
    F_p; F_n \rightarrow k2 F_n\) where \(k_1,k_2 > 0\). This property
    shows measure uniformity with respect to proportional changes in
    classification results. If the measure is non-invariant, then its
    applicability may depend on class size.

 \end{itemize}



%----------------------------------------
% Scalar Evaluation Methods
%----------------------------------------
\section{Scalar Evaluation Methods}
Although the \textit{confusion matrix} shows
all of the information about the classifier's performance at a
particular operating point, more
meaningful measures can be extracted from it to illustrate certain
performance criteria. Such measures are described in
this section and their suitability in different domains analyzed with
respect to the invariance properties described above.

\subsection{Precision and Recall}
In applications where the focus is on one particular class and the
number of examples belonging to that class are substantially lower
than the overall number of examples, the measures of
choice, calculated on the positive class, are \textit{recall} and
\textit{precision}.

\begin{equation}
\label{equ:precision}
precision = \frac{T_p}{T_p + F_p}
\end{equation}

\begin{equation}
\label{equ:recall}
recall = \frac{T_p}{T_p + F_n}
\end{equation}

\textit{F-score} is a derivative of \textit{precision} and
\textit{recall}. It is evenly balanced between the two in the case
where \(\beta = 1\). It favours \textit{precision} where \(\beta > 1\)
and \textit{recall} otherwise.

\begin{equation}
\label{equ:fscore}
Fscore = \frac{T_p}{T_p + F_n}
\end{equation}

\textit{F-score}, \textit{precision}, and \textit{recall} are
invariant under property \textbf{p2}. This property makes them a tool
of choice in measuring performance in text classification, information
extraction and natural language processing.


\subsection{Sensitivity and Specificity}
While \textit{accuracy} does not distinguish between the number of
correct labels of different classes, \textit{sensitivity} and
\textit{specificity} estimate a classifier's performance correctly
labelling positive and negative instances respectively:

\begin{equation}
\label{equ:sensitivity}
sensitivity = \frac{T_p}{T_p + F_n}= recall
\end{equation}

\begin{equation}
\label{equ:recall}
specificity = \frac{T_n}{F_p + T_n}
\end{equation}

\textit{Specificity} and \textit{sensitivity} are often employed in
biomedical and medical applications, and in studies involving image and
visual data. Various derivatives of \textit{specificity} and
\textit{sensitivity} are used in medical diagnosis to analyze tests \cite{RefWorks:37}:

\begin{itemize}
  \item \textbf{Youden's Index:} evaluates an algorithm's ability to
    avoid failure, equally weighting its performance on positive and
    negative examples.

    \begin{equation}
      \label{equ:youden}
      \gamma = sensitivity - (1 - specificity)
    \end{equation}
    
    \item \textbf{Likelihood:} accommodates both \textit{sensitivity}
      and \textit{specificity}, but treats them separately. This makes
      it possible to evaluate the classifier's performance to a finer
      degree with respect to both classes. 

      \begin{equation}
        \label{equ:likelihoodp}
        \rho_+ = \frac{sensitivity}{1 - specificity}
      \end{equation}

      \begin{equation}
        \label{equ:likelihoodn}
        \rho_- = \frac{1 - sensitivity}{specificity}
      \end{equation}

      A higher positive and lower negative \textit{likelihood} means
      better performance on positive and negative classes
      respectively. 

      \item \textbf{Discriminant Power:} is another measure
        summarizing \textit{sensitivity} and \textit{specificity}:

        \begin{equation}
          \label{equ:dp}
          DP = \frac{\sqrt{3}}{\pi}(\log{X} + \log{Y})
        \end{equation}

        \begin{equation}
          \label{equ:xy}
          X = \frac{sensitivity}{1 - sensitivity}; Y =
          \frac{specificity}{1 - specificity}
        \end{equation}
        
        DP evaluates how well an algorithm distinguishes between
        positive and negative examples and mostly used for feature
        selection in machine learning \cite{RefWorks:37}. The
        algorithm is a poor discriminant if \(DP < 1\), limited if
        \(DP < 2\), fair if \(DP < 3\), and good in other
        cases.

\end{itemize}

\textit{Specificity}, \textit{Youdon's Index}, \textit{Likelihood},
and \textit{Discriminant Power} are non-invariant for all invariance
properties described above. \textit{Sensitivity} is non invariant for
\textbf{p2} and \textbf{p2}.

%----------------------------------------
% Non-Scalar Evaluation Methods
%----------------------------------------
\section{Non-Scalar Evaluation Methods}
The measures of performance discussed so far are valid only for one
particular \textit{operating point}, an operating point normally being
chosen so as to minimize the \textit{probability of error}. However,
in general it is not misclassification rate we want to minimize, but
rather \textit{misclassification cost}. \textit{Misclassification
  cost} is usually defined as follows:

\begin{equation}
\label{equ:misclasscost}
Cost = F_p * {C_F}_p + F_n * {C_F}_n
\end{equation}

where \(F_p\) and \(F_n\) are the false positive and false negative
counts respectively, and \({C_F}_p\) and \({C_F}_n\) are the cost of a
false positive and false negative classification respectively. Class
imbalance and asymmetric misclassification costs are related. It has
been suggested that training set imbalances may be addressed by
training a cost sensitive classifier with the misclassification cost
of the minority class greater than that of the majority class, and one
way to make an algorithm cost sensitive is to synthetically imbalance
the training set to reflect the cost ratio
\cite{RefWorks:52}. Unfortunately, little work has been published on
either problem \cite{RefWorks:61}.

The specification of a particular set of
misclassification costs or false and positive misclassification rates
define a particular \textit{operating point} of a classification system
Non-scalar evaluation methods try to characterize the performance of a
classifier over a range of operating conditions. Although this makes
direct performance comparisons ambiguous, we are more likely to be
able to select the classifier which will perform best in the real
world by applying domain expertise to the results of a non-scalar
evaluation. Because non-scalar measures incorporate multiple
\textit{operating points}, they can compensate the effects of
invariance properties described in section \ref{sec:confusionmatrix}.

Despite the benefits of non-scalar
evaluation techniques, there is still a need for a single measure of
classifier performance that is invariant to the decision criteria, and
is easily extended to include cost/benefit analysis
\cite{RefWorks:32}. In some cases non-scalar evaluation techniques
have scalar derivatives which allow unqualified comparison of
classifiers and qualitative comparison of variances between scalar and
non-scalar metrics.

There seems to be growing recognition in the field of the limitations
of scalar evaluation techniques and increased use and development of
non-scalar methods. Below we consider important non-scalar evaluation
techniques:

\subsection{ROC Curves}
The \textit{ROC} curve was first developed by electrical and radar
engineers during World War II for detecting enemy objects in battle
fields and more recently, it has been employed in the medical decision
making community. In addition to being a useful performance graphing
method, \textit{ROC} curves exhibit useful properties that make them
especially useful for domains with skewed class distributions and
unequal classification cost errors \cite{RefWorks:39}. 

The curve is a plot of the \textit{sensitivity}, or \textit{true
  positive rate}, vs. \textit{false positive rate} (\(1 âˆ’
specificity\) or \(1 - true negative rate\)), for a binary classifier
system as its discrimination threshold is varied (each threshold
defines a classifier).

\begin{equation}
\label{equ:truepositiverate}
TP = \frac{T_p}{T_p + F_n}
\end{equation}

\begin{equation}
\label{equ:falsepositiverate}
TN = \frac{F_p}{T_n + F_p}
\end{equation}

The sample points obtained for various
discrimination thresholds are treated as points on a continuous curve
and normal curve fitting techniques applied. Together with the sample
points obtained during testing, the points \((0,0)\) and \((1,1\) are
included in the curve plot. The lower left point (\(0,0)\) represents
the strategy of never issuing a positive classification. The opposite
strategy of unconditional positive classification is represented by the
upper right hand point \((1,1)\). For classification algorithms which
do not use an explicit decision threshold, like Decision Trees, class
frequencies in the training set can be changed by under or
oversampling to simulate a change in class priors or misclassification
costs. 

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth,height=!]{roc}
  \end{center}
  \caption{ROC Curves}
  \label{fig:roc}
\end{figure} 

If the proportion of positive and negative instances changes in a test
set, the \textit{ROC} curve remains the same. This is evident from the
\textit{confusion matrix} shown in Figure
\ref{tab:confusionmatrix}. \textit{ROC} curves are based on
\textit{true positive rate} (\ref{equ:truepositiverate}) and
\textit{false positive rate} (\ref{equ:falsepositiverate}) which are
ratios, normalized for class distribution. Given the relationship
between class distribution and error cost, which we explore below, we
can say that \textit{ROC} curves are invariant with respect to the
operating conditions (class skew and error cost). This is a desirable
property for performance analysis since as operating conditions
change, the region of interest on the graph may change, but not the
graph itself.

Figure \ref{fig:roc} shows a sample
\textit{ROC} curve for three classification systems (A, B and C). The
line \(TP=FP\) represents the performance of a random
classifier which exploits no information in the training data. Any
classifier that produces a point below this line performs worse than
random guessing. Such a classifier is said to have useful information
but is applying it incorrectly and the sign of the classifier can be
negated to produce a useful \textit{operating point} \cite{RefWorks:60}.

\subsection{Comparing Classifiers}
\textit{ROC} curves may be overlaid for comparison. One
point in the diagram is better than another if it is above and to the
left (northwest)
of the other point i.e. has a higher true positive and a lower false positive
rate. Classifiers appearing on the left hand side of the graph near
the \(X\) axis, make positive
classifications only with strong evidence. Such classifiers make few
false positive errors but often have low true positive rates as
well. Classifiers on the upper right hand side of the graph make
positive classifications with weak evidence. Such classifiers will
classify nearly all positives correctly but will have a high false
positive rate \cite{RefWorks:39}. With respect to Figure \ref{fig:roc},
curve \(A\) represents a dominant classifier system, clearly bettering
systems \(B\) and \(C\), between which there is no clear winner. 

\textit{ROC} curves show the ability of a classifier to rank the
positive instances relative to the negative ones. The scores produced
by a \textit{probabilistic} classifier are relative to other instances
only and not properly \textit{calibrated} as true probabilities
are. For this reason, classifier scores should not be compared across
model classes and comparing model performance at a common threshold is
meaningless \cite{RefWorks:39}. 

Provost and Fawcett (\cite{RefWorks:61}) describe techniques for
comparing the performance of a set of learned classifiers. These
techniques are based on the principles of separation of classifier
performance from class and cost distribution and the use of a convex
hull to identify potentially optimal classifier subsets. These
techniques were examined further by Drummond and Holte
(\cite{RefWorks:52}) who describe an alternate projection of the
\textit{ROC} plot in which the expected cost of a classifier is
represented explicitly. These treatments are described in more detail
below: 


\subsubsection{Iso-performance Lines}
The expected cost of applying the classifier represented by a point
\((FP, TP\) in \textit{ROC} space  is:

\begin{equation}
\label{equ:expectedcost}
p(+)(1-TP){C_F}_n + p(-)FP{C_F}_p
\end{equation}

Where \(p(+)\) and \(p(-) = 1 - p(+)\), represent the classes' prior
distribution. Therefore, two points \((FP_1, TP_1\) and \((FP_2,
TP_2\), have the same performance if 

\begin{equation}
  \frac{TP_2 - TP_1}{FP_2 - FP_1} = \frac{{C_F}_p . p(+)}{{C_F}_n . p(-)}
\end{equation}

This equation defines the slope of an \textit{iso-performance
  line}. All classifiers corresponding to points on the line have the
same expected cost. Lines having larger \(TP\)-intercept correspond to
a lower expected cost \cite{RefWorks:61}.

In Figure \ref{fig:roc}, point \(a\) on curve \(A\) represents the
intercept of an \textit{iso-performance line} with gradient
\(3/2\). This corresponds to a scenario in which negatives outnumber
positives by 3 to 2 but false positives and false negatives have equal
cost. Since the classifier on curve \(A\) at point \(a\) is the most
northwest line of slope \(3/2\), it represents the best classifier for
these conditions. Each set of class and cost distribututions defines a
family of \textit{iso-performance} lines.


\subsubsection{\textit{ROC} Convex Hull (ROCCH)}
Given that in the real world, the operating conditions within a
problem domain are seldom known precisely, it is useful to be able to
identify those classifiers which are potentially optimal
\cite{RefWorks:61}. If a collection \textit{ROC} curves are plotted
together, the \textit{convex hull} of the set of points has important
properties. A classifier is potentially optimal if and only if it lies
on the \textit{convex hull} of this set of points in \textit{ROC}
space. Intuitively, the \textit{convex hull} represents the northwest
boundary of the set of points in \textit{ROC} space and, as discussed
above, the locus of the strongest classifiers. An \textit{ROC} curve
with no points on the \textit{convex hull} is not optimal for any
operating conditions. In Figure \ref{fig:roc}, curve \(A\) alone forms
the convex hull of the collection of points so it is potentially
optimal for all operating conditions. Curves \(B\) and \(C\) are
clearly suboptimal for all operating conditions.

As discussed above, a particular operating point defines the slope of
an \textit{iso-performance line} and the \textit{iso-performance line}
with the lowest expected cost is that with the highest
\(TP\)-intercept. It is easy to see that optimal performance for these
operating conditions is achieved by the classifier on the
\textit{ROCCH}, tangent to this \textit{iso-performance line} line. In
Figure \ref{fig:roc} this corresponds to classifier \(a\).


\subsubsection{Area Under the \textit{ROC} Curve (AUC)}
In order evaluate the \textit{ROC} curve as a whole i.e. extract a
single distinguishable scalar metric, the area under the \textit{ROC}
curve, \textit{AUC} seems to exhibit desirable
properties. \textit{AUC} represents the probability that a randomly
chosen positive example is correctly ranked with greater suspicion
than a randomly chosen negative example. \cite{RefWorks:32} found good
agreement between \textit{accuracy} and \textit{AUC} with respect to
performance ranking as well as the following desirable properties:

\begin{itemize}
\item Increased sensitivity in the Analysis of Variance (ANOVA) tests.
\item It is not dependent on the decision threshold chosen.
\item It is invariant to prior class probabilities.
\end{itemize}

The \textit{AUC} defined by a single classifier test run is widely
known as \textit{balanced accuracy}:

\begin{equation}
  AUC_b = \frac{sensitivity + specificity}{2}
\end{equation}

Ling et al \cite{RefWorks:56} showed empirically and formally that
\textit{AUC} is statistically consistent and a more discriminating measure
than \textit{Accuracy}. Also, if we build classifiers that optimize
\textit{AUC} instead of \textit{accuracy}, it has been shown
\cite{RefWorks:62} that such classifiers produce not only better
\textit{AUC} but also better \textit{accuracy}, compared to
classifiers which optimize \textit{accuracy}. Huang and Ling
\cite{RefWorks:42} proposed a new composite scalar measure,
\textit{AUC:acc} combining \textit{AUC} and \textit{accuracy}. They
show formally that the new measure is consistent with and finer than
the existing measures it combines.

It is not possible with a single scalar metric to capture
entirely the performance of a classification system over a range of
operating conditions. It is possible for a high \textit{AUC}
classification system to perform worse than a low
\textit{AUC}. However, in practise the \textit{AUC} is a strong
predictor of performance and is often used when a general measure of
predictiveness is required \cite{RefWorks:39}.


\subsubsection{Cost Curves}
Drummond and Holte (\cite{RefWorks:52} describe a technique called
\textit{cost curves} to facilitate quantifying the performance
difference between two \textit{ROC} curves. In \textit{ROC} space, the
performance difference between two classifiers is not the Euclidean
distance normal to the lower curve but weighted linear function
incorporating the operating conditions. They argue that while it
is possible to obtain this information from \textit{ROC} curves
directly, it is not trivial. A \textit{cost curve} is a transform of
the \textit{ROC} space, plotting a probability cost function (Equation
\ref{equ:probcostfunc}) on the \(x\)-axis, against the expected cost
normalized with respect to the cost incurred when every example is
incorrectly classified. 

\begin{equation}
\label{equ:probcostfunc}
\frac{p(+){C_F}_n}{p(+){C_F}_n + p(-){C_F}_p}
\end{equation}

Each point in the \textit{cost curve} corresponds to an
\textit{iso-performance line} in \textit{ROC} space. The probability
cost function of the \textit{cost curve} governing the slope of the
\textit{iso-performance line} and the normalized expected cost, the
\(TP\) intercept. The distance between two cost curves directly
indicates the performance between the two. 

\textit{Cost curves} are no more powerful than \textit{ROC} curves,
offering only an alternative representation of the results of
performance testing that explicitly represents cost differences
between classifiers. While this representation is useful, it is this
authors opinion that its primary motivation is the explicit
quantification of performance differentials. As discussed previously,
performance evaluation is a statistical estimation exercise and should
be used qualitatively to select one classifier as optimal over
another, not to quantify said performance differential.

%----------------------------------------
% Statistical Techniques
%----------------------------------------
\section{Statistical Techniques}
In most cases the true distribution of examples to which a classifier
will be applied is not known in advance. However, to make informed decisions
regarding selection of classifiers,
performance must be estimated using the data available
\cite{RefWorks:45}. Application of statistical techniques is
becoming  widespread in the field of machine learning and serve to
formalize the field of machine learning and bring it
 in line with more established fields of diagnostics and
classification. Below we discuss some of the techniques employed in
machine learning evaluation:

\subsection{Sub-sampling}
Ultimately all machine learning evaluation techniques use a finite set
of training samples to project the performance characteristics of a
classifier over unseen data. It is known that single train and test
partitions are not reliable estimators of true error rate of a
classification scheme on a limited data set \cite{RefWorks:57}. In
addition to selecting an appropriate performance metric, it important
to employ some random sub-sampling scheme to minimize any estimation
bias. \textit{Cross-validation} is a common sub-sampling technique
used in machine learning evaluation.

Scalar metrics obtained from sub-sampling are easily combined into
means and standard deviations. Non-scalar metrics are not so easily
combined and the best method depends on the metric itself as well as
the specific goals of the study. During a
study by Bradley (\cite{RefWorks:32}) using \textit{ROC} curves, it was found
that averaging raw data (frequencies of true and false positives) from
\textit{cross-validation} to produce plots,
depressed the combined index of \textit{accuracy} and \text{AUC}. In the same study,
Bradley selected a technique known as \textit{pooling} to combine the
results of cross-validation to generate ROC curves. Foster et al
(\cite{RefWorks:45} used an alternative methodology, called
\textit{averaging}, after considering Bradley's methods inappropriate
for their requirements.


\subsection{Analysis of Variance}
Comparison of experimental results over various data sets to determine
whether the difference between two classifiers is non-random, is usually
performed using paired statistical tests. Specific tests used include the
\textit{t-test}, \textit{Wilcoxon test}, \textit{bootstrap} method,
or \textit{Analysis of Variance} (ANOVA).

The \textit{t-test} makes the assumption that the population to which
it is applied is \text{normally distributed}. In cases where this assumption
does not hold, it is possible for the \textit{t-test} to give the same
confidence value to an erratic as to a non-erratic classifier
\cite{RefWorks:43}. 

The \textit{Wilcoxon test} is a non-parametric alternative to the
\textit{t-test} which ranks the difference in performance of two
classifiers for each data set. It is safer than the \textit{t-test}
since it does not assume a \textit{normal distribution}
\cite{RefWorks:47}. 

Studies conducted by \cite{RefWorks:38} examined
the use of \textit{ANOVA} (Analysis of Variance) to determine the
significance of means of \textit{AUC} in Monte Carlo experiments for a
variety of machine learning algorithms. It was indicated that since ANOVA does not
estimate case-sample variance, it tends to be 
optimistic, especially for large samples. While study was based on
\textit{normal distributions}, and therefore not generally applicable, it does
suggest that problem domain specifics should be taken into account
when using ANOVA.

Goutte and Gaussier \cite{RefWorks:40} examined the variance of
\textit{precision}, \textit{recall}, and {F-score} and used the
\textit{bootstrap} method to derive approximate confidence intervals
for different point estimates. It was found that the performance
metrics do not always correspond to sample means or medians and the
\textit{bootstrap} method may fail to give accurate confidence
intervals. They propose a probabilistic framework which takes into
account the intrinsic variability of performance estimation in
comparing performance metrics.


%----------------------------------------
% Benchmark Data Sets
%----------------------------------------
\section{The Real World}
While the selection of appropriate metrics and statistical techniques
are important in evaluating and selecting classifiers, it is important
to remember that our application cannot be expected to perform well in
the real world if the training data does not adequately represent the
problem domain. Very few systems generated with machine learning have
been deployed and used in the field for a sizable period of time
\cite{RefWorks:46}. Part of the reason for this is that the users of
the system and domain experts must be convinced that the system will
perform well with actual data. Real world data from the field can be
surprisingly dirty \cite{RefWorks:58} and historical data used for
training may have been cleaned. It is common practise to use benchmark
data sets like those collected in the Irvine repository
\cite{RefWorks:59} for performance evaluation of machine learning
algorithms. While this practise provides important insights, it is
wrong to judge, on their basis, an algorithm's practical adequacy
\cite{RefWorks:46}.

Comprehensibility goes a long way to building confidence among
stakeholders of a classifiers capabilities, however as mentioned
previously, few machine learning models offer much in this respect.

\section{Conclusions}
Machine Learning is a relatively new field that is rapidly
maturing. Initial work in the field made rapid advances that were
evident even in the face rudimentary evaluation measures. It was not
difficult to tell if a particular avenue of research would yield
benefits or not. However, as the field matures and the low hanging
fruit is plucked from the tree, it is becoming more difficult and
important to be able measure and have confidence of the abilities of
machine learning systems relative to one another and prevailing system
in use in the real world.

In this paper we have examined common measures used to evaluate
machine learning algorithms. From this examination it has been
determined that no single metric could be considered optimal for all
scenarios and selection of an appropriate metric for classifier
evaluation and/or training is as important as the machine learning
algorithm itself. Also, use of classifier performance metrics must be
complemented by statistical methods to build confidence in our
estimates and convince those in other fields of diagnostics and
decision making.

If one cannot discriminate between two classifier confidently, we
cannot hope to improve our approximation of the target function or
advance the state of the art.

%----------------------------------------
% Bibliography
%----------------------------------------
% changes default name Bibliography to References
\renewcommand{\bibname}{References}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibliography}

%----------------------------------------
\end{document}
